---
title: "p8105_hw5_xf2302"
author: "Xiuhong Fan"
date: "2025-11-13"
output: github_document
---
# Load Libraries and Set Seed
```{r}
library(tidyverse)
library(broom)
library(purrr)
library(readr)
library(janitor)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

set.seed(8105)
```
# Problem 1
# Define Function to Check Shared Birthdays
```{r}
shared_bday = function(n) {
birthdays = sample(1:365, n, replace = TRUE)
any(duplicated(birthdays))
}
```
# Run Simulations for Group Sizes 2–50
```{r}
group_sizes = 2:50

results =
tibble(group_size = group_sizes) |>
mutate(prob_shared =
map_dbl(group_size,
~ mean(replicate(10000, shared_bday(.x))))
)
```
# Plot Probability vs. Group Size
```{r}
results |>
ggplot(aes(group_size, prob_shared)) +
geom_line() +
geom_point(alpha = .6) +
labs(
title = "Probability of Shared Birthday",
x = "Group Size",
y = "Probability"
)
```

The probability of shared birthdays is very low for small groups but increases quickly as group size grows. Around 23 people, the probability passes 50%, confirming the birthday paradox. By 40–50 people, the probability is very high. The simulation matches the well-known pattern that shared birthdays occur more often than intuition suggests.

# Problem 2
# Design Parameters (n, σ) and Simulation Function
```{r}
n = 30
sigma = 5

sim_one = function(mu) {
  x = rnorm(n, mean = mu, sd = sigma)
  t_out = t.test(x, mu = 0)
  
  tibble(
    mu_hat = mean(x),
    p_value = t_out$p.value
  )
}
```
# Generate Simulation Grid and Run 5000 Iterations
```{r}
mu_values = 0:6
n_iter = 5000

sim_results =
  expand_grid(
    mu = mu_values,
    iter = 1:n_iter
  ) |>
  mutate(
    results = map(mu, sim_one)
  ) |>
  unnest(results)
```
# Plot: Power vs Effect Size
```{r}
power_df =
  sim_results |>
  mutate(reject = p_value < 0.05) |>
  group_by(mu) |>
  summarize(power = mean(reject))

power_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power vs Effect Size",
    x = "True μ",
    y = "Power (Pr Reject H0)"
  )
```

Power increases as μ increases because larger effect sizes give the test greater ability to detect a deviation from 0. When μ is small, rejection rates remain low, but as μ grows the null becomes easier to reject and power rises toward 1.

# Plot: Average μ̂ Across All Samples
```{r}
sim_results |>
  group_by(mu) |>
  summarize(avg_mu_hat = mean(mu_hat)) |>
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimated μ̂ vs True μ",
    x = "True μ",
    y = "Average μ̂"
  )
```

# Plot: Average μ̂ for Rejected Samples Only
```{r}
sim_results |>
  mutate(reject = p_value < 0.05) |>
  filter(reject) |>
  group_by(mu) |>
  summarize(avg_mu_hat_rejected = mean(mu_hat)) |>
  ggplot(aes(x = mu, y = avg_mu_hat_rejected)) +
  geom_point(color = "red") +
  geom_line(color = "red") +
  labs(
    title = "Avg μ̂ for Samples Where H0 Was Rejected",
    x = "True μ",
    y = "Average μ̂ (Rejected Only)"
  )
```

The overall average of μ̂ closely matches the true μ, showing that the estimator is unbiased. However, when considering only samples in which the null was rejected, the average μ̂ is higher than the true μ for small effect sizes. This occurs because significant results are more likely when μ̂ is an unusually large positive estimate, creating selection bias.